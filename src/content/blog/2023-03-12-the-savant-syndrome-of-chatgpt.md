---
title: The Savant Syndrome of ChatGPT
date: 2023-03-12T19:00:40.086Z
slug: 2023-03-12-the-savant-syndrome-of-chatgpt
author: Thomas Wilson

---
As I write this (March 2023), we're (hopefully) past peak-hype for ChatGPT, and into the phase where we start to find the boundaries and limitations.  Or at least temporary limitations before the next language model addresses them.

As a very up-front statement of a dystopian future: large language models (like the ones OpenAI are wielding) might bring  some fresh horror for our already technologically-hobbled society, and potentially end us (or cause us to end ourselves) because they let people bypass the barriers to access to worryingly dangerous information (like bioweapons).  We really, really should be taking these things seriously.

OpenAI's ChatGPT app (and now API) is insanely cool, but in a specific way.

I've been using it to help with both technical and administrative tasks, and it does some things _really_ well:

- It can sketch out code _super_ quickly: an interface or class.
- It can iterate on those sketches really well: give it some feedback about a potential weakness, and it will address those.
- It can workshop ideas: if you describe a problem, and ask it to suggest a couple of alternative names for things.
- It can re-word (re-phrase, summarise) natural language: this is especially useful if you ever need to use corporate speak.

At the moment, ChatGPT can do the first pass on these things faster than I can.  Way faster.  And get 80-90% of the way there, in maybe 10-100x the time, it's great.  The breadth of knowledge is also stunning - it can suggest synonyms or related concepts. 

What's more, it can then explain that new idea or word to you... in the exact same place as it was introduced.  It's ripe for rabbit holes.  It's the lowest friction wikipedia journey you ever went on.  You can keep asking "tell me more about that" and it just... does.

However the deeper into a problem you, the more specifics you have - the harder it is to get useful advice.  For example, I asked how I could get the first 100 users for a certain kind of app (say a Language Learning app), and it told me to:

1. Ask my friends (good for very early feedback)
2. Post on social media (sure, but what?)
3. Partner with language learning schools (are enterprise customers good early customers?)
4. Offer a special promotion (to who?  The void of users I don't have?)
5. Attend conferences (pretty good idea)
6. Join online communities (spam strangers on the internet?)

Acquiring your first 100 users is pretty hard, there are a lot of books and talks and opinions about it.  It's also a pretty noisy signal: it's famously hard to know if someone's advice is true, or if they're just accidentally successful and doing a figurative rain dance. Some people succeed _despite_ their early decisions, others _because_.  Some people are just lucky.  People get a lot of praise when they tell others how to become successful.

People have spoken about Large Language Models' tendency to hallucinate (i.e. report something as true even though it never happened), or to be confidently wrong. 

This is like that, but different - there's a lack of awareness of wider context, but also a lack of awareness that there can even _be_ a wider context that matters.  

This is [Savant Syndrome](https://en.wikipedia.org/wiki/Savant_syndrome) - ChatGPT has exceptional abilities in some places, but lacks "basic" skills.

A few times when I was co-designing some code with ChatGPT it would present me with some very specific code to the exact problem we were discussing.  It wasn't able to confidently generalise some ideas, which meant that certain ideas were hard-coded into the code.   

Designing good software is all about designing code that can change in the future, and knowing where that can cause the most pain.  That's the reality of being a software engineer, and it feels pretty fundamental to me.

I found ChatGPT most useful as a tool to reduce the friction of resolving specific bumps along the way to solving a problem.  If I needed a term defined, or needed a basic interface in TypeScript sketched out, or if I wanted some example scenarios - I was able to get them there-and-then.  

What's perhaps worrying is how similar this is TikTok's content model (a fire hose).  Unlike TikTok's content, ChatGPT is very obviously computer-generated, which I think will encourage people to trust it more.

Your mileage may vary, proceed with caution, use your common sense, so on and so forth.